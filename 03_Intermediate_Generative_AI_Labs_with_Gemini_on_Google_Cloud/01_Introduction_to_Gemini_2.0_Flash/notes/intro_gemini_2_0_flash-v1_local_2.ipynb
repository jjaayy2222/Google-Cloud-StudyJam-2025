{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbaca85c",
   "metadata": {},
   "source": [
    "### **멀티모달 프롬프트 전송**\n",
    "\n",
    "* *이어서 진행*\n",
    "* *befor: 로컬 이미지 전송*\n",
    "\n",
    "  ```\n",
    "  ↓\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1388835a",
   "metadata": {},
   "source": [
    "#### **Google Cloud Storage (`GCS`)** 에서 문서 보내기\n",
    "\n",
    "* 구글 클라우드 콘솔 실습에서 했던 내용\n",
    "  * 예시 문서는 Google과 토론토 대학교의 연구진이 작성한 논문인 [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762)\n",
    "  * `Gemini`를 활용한 문서 이해의 더 많은 예시를 보려면 다음 노트북을 확인할 것\n",
    "  * [Gemini를 활용한 문서 처리](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "* 로컬 환경에서 `GCS`에 접근하기 위한 **권한 문제**\n",
    "\n",
    "  * 구글 클라우드 콘솔에서는 이미 사용자의 계정 정보가 인증되어 있기 때문에 별도의 절차 없이 GCS에 파일을 업로드하거나 접근할 수 있음\n",
    "  * 하지만 로컬 컴퓨터에서는 구글 클라우드 환경과 분리되어 있어, GCS에 접근하기 위한 **인증 키나 권한 설정(Credential)**이 반드시 필요\n",
    "\n",
    "---\n",
    "\n",
    "* 로컬에서 `GCS`에 파일 업로드하는 방법\n",
    "\n",
    "  * **서비스 계정 키(Service Account Key)**: \n",
    "    * 로컬 개발 환경에서 가장 일반적인 방법\n",
    "    * 구글 클라우드에서 서비스 계정을 생성하고 키 파일을 다운로드하여 환경 변수로 설정하면 해당 계정의 권한으로 `GCS`에 접근 가능\n",
    "  * **개인 사용자 인증(User Authentication)**: \n",
    "    * `gcloud auth login` 명령어를 사용해 개인 계정으로 로그인하는 방법\n",
    "    * 로컬에서 실행되는 코드가 사용자의 계정 권한을 임시로 빌려 `GCS`에 접근 가능\n",
    "\n",
    "---\n",
    "\n",
    "* *재현하지 않고 다음으로 넘어감*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9509a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57b32c",
   "metadata": {},
   "source": [
    "### 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedbb069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬에서 실행 시\n",
    "from IPython.display import Markdown, display, HTML\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()   \n",
    "\n",
    "# 클라이언트 초기화\n",
    "# 단일 클라이언트 객체 생성하기\n",
    "# API 키는 GEMINI_API_KEY 환경 변수에서 자동으로 로드\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "# 1. 사용할 모델 지정 (gemini-2.5-flash-lite)\n",
    "# 2. '헬로우' 프롬프트로 콘텐츠 생성 요청\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.5-flash-lite',                     # 사용할 모델 지정 (gemini-2.5-flash-lite)\n",
    "        contents='hello'                                   # 요청하신 프롬프트 '헬로우'\n",
    "    )\n",
    "\n",
    "    # 응답 텍스트 출력\n",
    "    print(\"\\n--- 모델 응답 텍스트 ---\")\n",
    "    print(response.text)\n",
    "\n",
    "    # 응답 전체 내용(JSON 형식) 출력 (디버깅이나 상세 정보 확인용)\n",
    "    print(\"\\n--- 모델 응답 전체 JSON ---\")\n",
    "    print(response.model_dump_json(\n",
    "        exclude_none=True, indent=4))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n모델 호출 중 오류가 발생했습니다: {e}\")\n",
    "    print(\"다음 사항들을 확인해주세요:\")\n",
    "    print(\"1. 인터넷 연결 상태\")\n",
    "    print(\"2. GEMINI_API_KEY 환경 변수가 올바르고 유효한지\")\n",
    "    print(\"3. Google Cloud 프로젝트에서 Gemini API가 활성화되어 있는지\")\n",
    "    print(\"4. API 할당량이 초과되지 않았는지\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf82bb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145f78f",
   "metadata": {},
   "source": [
    "#### 일반 URL에서 오디오 보내기\n",
    "\n",
    "- 이 예시는 [`Kubernetes Podcast`](https://kubernetespodcast.com/) 에피소드의 오디오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티모달 프롬프트에 사용할 오디오 URL\n",
    "import requests\n",
    "import pathlib\n",
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "# Download file\n",
    "response = requests.get(\n",
    "    \"https://arxiv.org/html/1706.03762v7\")\n",
    "pathlib.Path('1706.03762v7.html').write_text(response.text)\n",
    "\n",
    "my_file2 = client.files.upload(file='1706.03762v7.html')\n",
    "\n",
    "try:\n",
    "    # 이미지와 텍스트를 포함한 멀티모달 프롬프트 전송\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.5-flash',\n",
    "        contents = [\"Summarize the document.\",\n",
    "                    my_file2]\n",
    "    )\n",
    "\n",
    "    # 생성된 텍스트 출력\n",
    "    print(response.text)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"이미지를 다운로드하는 중 오류가 발생했습니다: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"지정된 로컬 이미지 파일을 찾을 수 없습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류가 발생했습니다: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2baa819",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `pdf` 문서 (읽기 전용)이라 실패 → `HTML`로 도전 \n",
    "\n",
    "* 셀 출력 (원문: contents = \"Summarize the document.\")(30.3s)\n",
    "\n",
    "    ```markdown\n",
    "    This document introduces the **Transformer**, a novel neural network architecture for sequence transduction models, such as machine translation. Unlike previous dominant models that rely on complex recurrent (RNNs) or convolutional neural networks, the Transformer **exclusively uses attention mechanisms**, completely discarding recurrence and convolutions.\n",
    "\n",
    "    Key aspects and contributions of the Transformer:\n",
    "\n",
    "    1.  **Architecture:** It maintains an encoder-decoder structure. Both the encoder and decoder are composed of stacked identical layers, primarily featuring **Multi-Head Self-Attention** and position-wise fully connected feed-forward networks.\n",
    "        *   **Self-Attention** allows the model to weigh the importance of different parts of the input sequence when processing each element, enabling it to capture global dependencies without being limited by distance.\n",
    "        *   **Multi-Head Attention** allows the model to attend to information from different representation subspaces in parallel, providing a richer understanding of relationships.\n",
    "        *   **Positional Encodings** are added to input embeddings to incorporate information about the order and position of tokens in the sequence, a crucial addition since the model lacks intrinsic sequential processing.\n",
    "\n",
    "    2.  **Advantages over RNNs/CNNs:**\n",
    "        *   **Parallelization:** The attention-only architecture allows for significantly more parallel computation within training examples, reducing training time.\n",
    "        *   **Shorter Path Lengths:** It reduces the path length between any two positions in the input or output sequence to a constant, which is theorized to make learning long-range dependencies easier.\n",
    "        *   **Computational Efficiency:** It is often faster than recurrent layers when the sequence length is shorter than the representation dimensionality, common in many language tasks.\n",
    "\n",
    "    3.  **Performance:**\n",
    "        *   On the **WMT 2014 English-to-German translation task**, the Transformer achieved a new state-of-the-art BLEU score of 28.4, surpassing previous best results (including ensembles) by over 2 BLEU.\n",
    "        *   For the **WMT 2014 English-to-French translation task**, it set a new single-model state-of-the-art BLEU score of 41.8, while requiring a significantly lower training cost (3.5 days on eight GPUs) compared to existing top models.\n",
    "        *   The model also demonstrated good generalization by performing successfully on **English constituency parsing**.\n",
    "\n",
    "    In summary, \"Attention Is All You Need\" introduces a revolutionary architecture that leverages attention mechanisms as its sole building block, leading to substantial improvements in training speed and translation quality, setting a new standard for sequence transduction tasks.\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "* 셀 출력 (한국어 버전: contents = \"이 문서를 한국어로 요약해주세요\")(33.5s)\n",
    "\n",
    "    ```markdown\n",
    "    이 문서는 \"Attention Is All You Need\"라는 제목의 논문을 한국어로 요약한 것입니다. 이 논문은 기존의 복잡한 순환 신경망(RNN)이나 합성곱 신경망(CNN) 기반의 인코더-디코더 모델과 달리, 오직 어텐션 메커니즘에만 기반한 새로운 신경망 구조인 **트랜스포머(Transformer)**를 제안합니다.\n",
    "\n",
    "    **주요 내용:**\n",
    "\n",
    "    1.  **배경 및 문제점:**\n",
    "        *   기존 시퀀스 변환 모델(언어 모델링, 기계 번역 등)은 주로 RNN, 특히 LSTM이나 GRU에 기반하며, 인코더-디코더 구조에 어텐션 메커니즘이 추가된 형태입니다.\n",
    "        *   RNN은 순차적인 계산 방식 때문에 훈련 중 병렬화가 어렵고, 긴 시퀀스에서 장거리 의존성(long-range dependency)을 학습하는 데 어려움이 있습니다.\n",
    "        *   CNN 기반 모델(ByteNet, ConvS2S)은 병렬화가 가능하지만, 장거리 의존성을 모델링하려면 더 많은 레이어가 필요하여 경로 길이가 길어지는 한계가 있습니다.\n",
    "\n",
    "    2.  **모델 아키텍처 (Transformer):**\n",
    "        *   트랜스포머는 RNN이나 CNN 없이 **전적으로 어텐션 메커니즘**에 의존합니다.\n",
    "        *   **인코더-디코더 구조:** 기존 모델과 유사하게 인코더와 디코더 스택으로 구성됩니다.\n",
    "            *   **인코더:** N=6개의 동일한 레이어로 구성됩니다. 각 레이어는 Multi-Head Self-Attention 서브 레이어와 Position-wise Feed-Forward Network 서브 레이어로 이루어집니다.\n",
    "            *   **디코더:** N=6개의 동일한 레이어로 구성됩니다. 인코더 레이어와 동일한 두 서브 레이어 외에, 인코더의 출력을 대상으로 하는 세 번째 Multi-Head Attention 레이어를 포함합니다. 디코더의 Self-Attention은 미래의 정보를 참조하지 않도록 마스킹됩니다.\n",
    "            *   모든 서브 레이어에는 잔차 연결(Residual Connection)과 레이어 정규화(Layer Normalization)가 적용됩니다.\n",
    "        *   **어텐션 메커니즘:**\n",
    "            *   **Scaled Dot-Product Attention:** 쿼리(Query), 키(Key), 값(Value) 벡터를 입력받아, 쿼리와 모든 키의 내적을 계산하고, 이를 <img src=\"https://render.githubusercontent.com/render/math?math=%5Csqrt%7Bd_k%7D\" alt=\"sqrt{d_k}\">로 스케일링한 후 소프트맥스를 적용하여 값에 대한 가중치를 얻습니다.\n",
    "            *   **Multi-Head Attention:** 단일 어텐션 함수 대신, 쿼리, 키, 값을 여러 개의 다른 학습된 선형 투영을 통해 작은 차원의 여러 \"헤드\"로 분할한 다음, 각 헤드에서 병렬로 어텐션 함수를 수행합니다. 이를 통해 모델이 여러 위치에서 다양한 표현 부분 공간의 정보에 동시에 집중할 수 있습니다. (논문에서는 h=8개의 헤드 사용)\n",
    "            *   **모델 내 어텐션 활용:**\n",
    "                *   **인코더-디코더 어텐션:** 디코더가 인코더의 모든 입력 시퀀스 위치를 참조할 수 있도록 합니다.\n",
    "                *   **인코더 셀프-어텐션:** 인코더 내에서 각 위치가 이전 레이어의 모든 위치를 참조할 수 있도록 합니다.\n",
    "                *   **디코더 셀프-어텐션:** 디코더 내에서 각 위치가 자기 자신을 포함하여 이전까지 생성된 모든 위치를 참조할 수 있도록 마스킹됩니다.\n",
    "        *   **위치 인코딩(Positional Encoding):** 순환이나 합성곱이 없으므로, 시퀀스의 순서 정보를 모델에 주입하기 위해 입력 임베딩에 \"위치 인코딩\"을 더합니다. 이는 다양한 주파수의 사인 및 코사인 함수를 사용하여 고정된 방식으로 계산됩니다.\n",
    "\n",
    "    3.  **셀프-어텐션의 장점:**\n",
    "        *   **계산 효율성:** 각 레이어당 계산 복잡도가 RNN 레이어에 비해 효율적입니다 (특히 시퀀스 길이 <img src=\"https://render.githubusercontent.com/render/math?math=n\">이 표현 차원 <img src=\"https://render.githubusercontent.com/render/math?math=d\">보다 작을 때).\n",
    "        *   **병렬화:** 순차적 계산이 O(1)에 불과하여 훈련 시 높은 병렬화를 가능하게 합니다.\n",
    "        *   **장거리 의존성 학습:** 네트워크 내에서 입력 및 출력 위치 간의 최대 경로 길이가 상수(O(1))로 매우 짧아, 장거리 의존성 학습에 매우 유리합니다.\n",
    "        *   **해석 가능성:** 어텐션 헤드들이 문장의 구문적, 의미적 구조와 관련된 다양한 역할을 학습함을 발견했습니다.\n",
    "\n",
    "    4.  **훈련 및 결과:**\n",
    "        *   **훈련 데이터:** WMT 2014 영어-독일어 (약 450만 문장 쌍) 및 영어-프랑스어 (약 3,600만 문장 쌍) 데이터셋.\n",
    "        *   **훈련 비용:** 8개의 NVIDIA P100 GPU를 사용하여 기본 모델은 12시간, 대형 모델은 3.5일 훈련.\n",
    "        *   **최적화:** Adam 옵티마이저를 사용하며, 학습률은 웜업(warmup) 스텝 이후 역제곱근에 비례하여 감소합니다.\n",
    "        *   **정규화:** 잔차 드롭아웃(Residual Dropout)과 레이블 스무딩(Label Smoothing)을 적용했습니다.\n",
    "        *   **성능:**\n",
    "            *   **기계 번역:** WMT 2014 영어-독일어 번역 태스크에서 28.4 BLEU를 달성하여 기존 최고 성능 모델(앙상블 포함)보다 2 BLEU 이상 뛰어난 **최고 성능(State-of-the-Art, SOTA)**을 기록했습니다. 영어-프랑스어 번역에서도 41.8 BLEU로 단일 모델 최고 성능을 달성했으며, 훈련 비용은 기존 SOTA 모델의 일부에 불과했습니다.\n",
    "            *   **영어 구성 요소 구문 분석:** 특정 태스크에 대한 튜닝 없이도 기존의 RNN 기반 모델보다 우수한 성능을 보여주며 다른 태스크로의 우수한 일반화 능력을 입증했습니다.\n",
    "\n",
    "    **결론:**\n",
    "\n",
    "    트랜스포머는 어텐션에 전적으로 기반한 최초의 시퀀스 변환 모델로서, 기존 순환 및 합성곱 모델보다 훨씬 빠른 훈련 속도를 제공하면서도 기계 번역 등 다양한 태스크에서 뛰어난 품질과 새로운 SOTA 성능을 달성했습니다. 저자들은 어텐션 기반 모델의 미래에 대한 기대를 표하며, 텍스트 외 다른 모달리티(이미지, 오디오, 비디오)로의 확장과 비순차적 생성 연구를 계획하고 있습니다. 모델의 코드는 텐서플로우(TensorFlow) 깃허브 저장소에 공개되어 있습니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8165f5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a605791",
   "metadata": {},
   "source": [
    "#### `YouTube` `URL`에서 동영상 보내기\n",
    "\n",
    "- 이 예시는 [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM) 유튜브 동영상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티모달 프롬프트에 사용할 유튜브 주소 = \"https://www.youtube.com/watch?v=3KtWfp0UopM\"\n",
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "# URL 정보를 딕셔너리로 직접 전달\n",
    "video_content = {\n",
    "    \"uri\": \"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
    "    \"mime_type\": \"video/mp4\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 이미지와 텍스트를 포함한 멀티모달 프롬프트 전송\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.5-flash',\n",
    "        contents = [\"At what point in the video is Harry Potter shown?\",\n",
    "                    video_content],\n",
    "    )\n",
    "\n",
    "    # 생성된 텍스트 출력\n",
    "    print(response.text)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"이미지를 다운로드하는 중 오류가 발생했습니다: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"지정된 로컬 이미지 파일을 찾을 수 없습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류가 발생했습니다: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66224b55",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (변수 조절 X)(21.3s)\n",
    "\n",
    "    ```markdown\n",
    "    Harry Potter is not explicitly shown in the video. However, two characters from the Harry Potter franchise are featured:\n",
    "\n",
    "    *   **Professor Snape** appears from **0:57 to 0:59**.\n",
    "    *   **Hagrid** appears from **0:59 to 1:02**.\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "* 셀 출력 (한국어로 묻기)(content: \"동영상에서 해리 포터가 언제 나오나요?\")(13.2s)\n",
    "\n",
    "    ```markdown\n",
    "    동영상에서 해리 포터와 관련된 장면은 **0:56초부터 1:02초** 사이에 나옵니다.\n",
    "\n",
    "    이때 'the most searched cast'라는 자막과 함께 스네이프 교수와 해그리드의 모습이 등장합니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897770a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc15a39",
   "metadata": {},
   "source": [
    "#### 멀티모달 라이브 API\n",
    "\n",
    "* `멀티모달 라이브 API`는 Gemini와 음성 및 영상으로 양방향 소통을 가능하게 하여, 대기 시간이 짧은(`low-latency`) 음성 및 영상 상호 작용을 지원 \n",
    "  * 이 `API`를 사용하면 자연스럽고 사람과 같은 음성 대화 경험을 최종 사용자에게 제공하고, 음성 명령을 사용하여 모델의 응답을 중단할 수도 있음. \n",
    "  * 이 모델은 `텍스트`, `오디오`, `영상` 입력을 처리하고, `텍스트`와 `오디오` 출력을 제공\n",
    "\n",
    "* `멀티모달 라이브 API`는 [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) 기반\n",
    "  * `멀티모달 라이브 API`의 더 많은 예시는 다음 [문서](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live) 또는 [노트북](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb)을 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8951e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 터미널에 사전 설치 : pip install pydantic\n",
    "\n",
    "# pydantic = 데이터 유효성 검사 및 설정 관리를 위한 파이썬 라이브러리 = BaseModel 사용 위해 필요\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response,'\\n')\n",
    "print('-'*50, '\\n')\n",
    "\n",
    "# 파싱된 Recipe 객체를 변수에 저장\n",
    "recipe_data = response.parsed\n",
    "\n",
    "# 깔끔하게 출력\n",
    "print(f\"--- 레시피 정보 ---\")\n",
    "print(f\"이름: {recipe_data.name}\")\n",
    "print(f\"설명: {recipe_data.description}\")\n",
    "print(f\"재료: \")\n",
    "for ingredient in recipe_data.ingredients:\n",
    "    print(f\"- {ingredient}\")\n",
    "print(f\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc363db",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (변수 조절 X)(1.3s)\n",
    "\n",
    "    ```markdown\n",
    "    sdk_http_response=HttpResponse(\n",
    "    headers=<dict len=11>\n",
    "    ) candidates=[Candidate(\n",
    "    content=Content(\n",
    "        parts=[\n",
    "        Part(\n",
    "            text=\"\"\"{\n",
    "    \"name\": \"Chocolate Chip Cookies\",\n",
    "    \"description\": \"A classic and beloved cookie, soft and chewy in the center with slightly crisp edges, loaded with chocolate chips.\",\n",
    "    \"ingredients\": [\n",
    "        \"All-purpose flour\",\n",
    "        \"Baking soda\",\n",
    "        \"Salt\",\n",
    "        \"Unsalted butter\",\n",
    "        \"Granulated sugar\",\n",
    "        \"Brown sugar\",\n",
    "        \"Eggs\",\n",
    "        \"Vanilla extract\",\n",
    "        \"Chocolate chips\"\n",
    "    ]\n",
    "    }\"\"\"\n",
    "        ),\n",
    "        ],\n",
    "        role='model'\n",
    "    ),\n",
    "    finish_reason=<FinishReason.STOP: 'STOP'>,\n",
    "    index=0\n",
    "    )] create_time=None response_id=None model_version='gemini-2.5-flash-lite' prompt_feedback=None usage_metadata=GenerateContentResponseUsageMetadata(\n",
    "    candidates_token_count=107,\n",
    "    prompt_token_count=11,\n",
    "    prompt_tokens_details=[\n",
    "        ModalityTokenCount(\n",
    "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
    "        token_count=11\n",
    "        ),\n",
    "    ],\n",
    "    total_token_count=118\n",
    "    ) automatic_function_calling_history=[] parsed=Recipe(name='Chocolate Chip Cookies', description='A classic and beloved cookie, soft and chewy in the center with slightly crisp edges, loaded with chocolate chips.', ingredients=['All-purpose flour', 'Baking soda', 'Salt', 'Unsalted butter', 'Granulated sugar', 'Brown sugar', 'Eggs', 'Vanilla extract', 'Chocolate chips']) \n",
    "\n",
    "    -------------------------------------------------- \n",
    "\n",
    "    --- 레시피 정보 ---\n",
    "    이름: Chocolate Chip Cookies\n",
    "    설명: A classic and beloved cookie, soft and chewy in the center with slightly crisp edges, loaded with chocolate chips.\n",
    "    재료: \n",
    "    - All-purpose flour\n",
    "    - Baking soda\n",
    "    - Salt\n",
    "    - Unsalted butter\n",
    "    - Granulated sugar\n",
    "    - Brown sugar\n",
    "    - Eggs\n",
    "    - Vanilla extract\n",
    "    - Chocolate chips\n",
    "    ------------------\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파싱된 Recipe 객체를 변수에 저장\n",
    "recipe_data = response.parsed\n",
    "\n",
    "# 깔끔하게 출력\n",
    "print(f\"--- 레시피 정보 ---\")\n",
    "print(f\"이름: {recipe_data.name}\")\n",
    "print(f\"설명: {recipe_data.description}\")\n",
    "print(f\"재료: \")\n",
    "for ingredient in recipe_data.ingredients:\n",
    "    print(f\"- {ingredient}\")\n",
    "print(f\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ffd2b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (변수 조절 X)(0.0s)\n",
    "\n",
    "    ```markdown\n",
    "    --- 레시피 정보 ---\n",
    "    이름: Chocolate Chip Cookies\n",
    "    설명: A classic and beloved cookie, soft and chewy in the center with slightly crisp edges, loaded with chocolate chips.\n",
    "    재료: \n",
    "    - All-purpose flour\n",
    "    - Baking soda\n",
    "    - Salt\n",
    "    - Unsalted butter\n",
    "    - Granulated sugar\n",
    "    - Brown sugar\n",
    "    - Eggs\n",
    "    - Vanilla extract\n",
    "    - Chocolate chips\n",
    "    ------------------\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b5454d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e857cd",
   "metadata": {},
   "source": [
    "### 생성된 출력 제어\n",
    "\n",
    "* [출력 제어(Controlled generation)](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output)는 응답 스키마를 정의\n",
    "  * `모델 출력의 구조`, `필드 이름` 및 `각 필드의 예상 데이터 유형`을 `지정`하는 기능입니다.\n",
    "  * 응답 스키마는 `config`의 `response_schema` 매개변수에 지정 → 모델 출력은 해당 스키마를 엄격하게 따름\n",
    "\n",
    "* 스키마 형태\n",
    "  * [Pydantic](https://docs.pydantic.dev/) \n",
    "  * [JSON](https://www.json.org/json-en.html) 문자열로 제공\n",
    "\n",
    "* 모델의 응답 :`response_mime_type`에 설정된 값에 따름\n",
    "  *  `JSON` \n",
    "  *  [Enum](https://docs.python.org/3/library/enum.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 SDK는 pydantic 클래스를 사용하여 스키마를 제공\n",
    "# genai.types.Schema 또는 이에 상응하는 dict을 전달할 수 있음\n",
    "# 가능하면 SDK는 반환된 JSON을 파싱하고 결과를 response.parsed로 반환\n",
    "# pydantic 클래스를 스키마로 제공한 경우 SDK는 해당 JSON을 클래스의 인스턴스로 변환\n",
    "\n",
    "from google import genai\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "class CountryInfo(BaseModel):\n",
    "    name: str\n",
    "    population: int\n",
    "    capital: str\n",
    "    continent: str\n",
    "    major_cities: list[str]\n",
    "    gdp: int\n",
    "    official_language: str\n",
    "    total_area_sq_mi: int\n",
    "\n",
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "    Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
    "\n",
    "    - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "    - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt,\n",
    "    config={\n",
    "        'response_mime_type': 'application/json',\n",
    "        'response_schema': response_schema,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c553c5",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (변수 조절 X)(1.7s)\n",
    "\n",
    "    ```markdown\n",
    "    [[{'rating': 4, 'flavor': 'Strawberry Cheesecake', 'sentiment': 'POSITIVE', 'explanation': \"The reviewer expresses strong positive sentiment, calling it the 'best ice cream ever'.\"}, {'rating': 1, 'flavor': 'Mango Tango', 'sentiment': 'NEGATIVE', 'explanation': \"Despite stating it's 'quite good', the reviewer finds it 'too sweet', resulting in a negative sentiment reflected in the very low rating.\"}]]\n",
    "    ````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d3cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 분석 결과 ---\n",
      "** 평점: 4\n",
      "** 맛: Strawberry Cheesecake\n",
      "** 감정: POSITIVE\n",
      "** 설명: The reviewer expresses strong positive sentiment, calling it the 'best ice cream ever'.\n",
      "---\n",
      "** 평점: 1\n",
      "** 맛: Mango Tango\n",
      "** 감정: NEGATIVE\n",
      "** 설명: Despite stating it's 'quite good', the reviewer finds it 'too sweet', resulting in a negative sentiment reflected in the very low rating.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# 중첩된 리스트 안에 딕셔너리가 들어가있는 결과값 → 반복문으로 꺼내기\n",
    "\n",
    "# response.parsed는 중첩 리스트이므로 첫 번째 요소를 가져오기\n",
    "parsed_reviews = response.parsed[0]\n",
    "\n",
    "print(\"--- 분석 결과 ---\")\n",
    "for review in parsed_reviews:\n",
    "    print(f\"** 평점: {review['rating']}\")\n",
    "    print(f\"** 맛: {review['flavor']}\")\n",
    "    print(f\"** 감정: {review['sentiment']}\")\n",
    "    print(f\"** 설명: {review['explanation']}\")\n",
    "    print(\"---\") # 각 리뷰를 구분하는 줄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad44f2b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e05bb79",
   "metadata": {},
   "source": [
    "### 토큰 개수 세기 및 계산\n",
    "\n",
    "* `count_tokens()` 메서드를 사용하면 Gemini API로 요청을 보내기 전에 입력 토큰 수를 계산할 수 있음\n",
    "  * 더 자세한 정보는 [토큰 나열 및 개수 세기](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) 문서를 참조\n",
    "\n",
    "* **[`SDK 이전 코드`](https://ai.google.dev/gemini-api/docs/migrate?hl=ko&_gl=1*10gkk0*_up*MQ..*_ga*MTk4MzUwNjQyNy4xNzU1NDE1MzE2*_ga_P1DBVKWT6V*czE3NTU0MTcxNDEkbzIkZzAkdDE3NTU0MTcxNDEkajYwJGwwJGgxNTcwMTgyOTg5#json-response) 우선 참조**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ebf00e",
   "metadata": {},
   "source": [
    "#### 토큰 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da58209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.count_tokens(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ff6c3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (변수 조절 X)(0.4s)\n",
    "\n",
    "    ```markdown\n",
    "    sdk_http_response=HttpResponse(\n",
    "    headers=<dict len=11>\n",
    "    ) total_tokens=10 cached_content_token_count=None\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "  * 제공해주신 응답은 **총 입력 토큰 수가 10개**라는 것을 의미\n",
    "\n",
    "    * **`total_tokens=10`**: 모델에 보낸 총 입력 토큰의 개수 = 이는 텍스트, 이미지, 동영상 등 모든 입력의 토큰을 합산한 값\n",
    "    * **`sdk_http_response`**: `SDK`가 `HTTP`를 통해 서버와 통신하며 받은 `응답의 세부 정보`\n",
    "    * **`cached_content_token_count=None`**: 캐시된 콘텐츠의 토큰 수가 없다는 의미 = 보통 동일한 프롬프트를 다시 보낼 때 나타날 수 있음\n",
    "\n",
    "    * 내가 모델에 보낸 `프롬프트(텍스트, 코드, 이미지 등)`가 **총 10개의 토큰으로 구성되어 있으며, 이 토큰 수를 기반으로 비용이 청구될 것**이라는 것을 알 수 있음\n",
    "\n",
    "  * `count_tokens()` 메서드 사용 →  `total_tokens` 값을 `API 호출 전`에 `미리 확인` 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caabd86",
   "metadata": {},
   "source": [
    "#### 토큰 계산\n",
    "\n",
    "* `compute_tokens()` 메서드는 `API 호출` 대신 `로컬 토크나이저를 실행` \n",
    "* 이 메서드는 `token_ids`(토큰 ID)와 `tokens`(실제 토큰)와 같은 더 자세한 토큰 정보를 제공\n",
    "\n",
    "* 실행 코드\n",
    "  \n",
    "    ```python\n",
    "    response = client.models.compute_tokens(\n",
    "        model=MODEL_ID,\n",
    "        contents=\"What's the longest word in the English language?\",\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "    ```\n",
    "\n",
    "<br>\n",
    "\n",
    "* 결과\n",
    "* \n",
    "    ```markdown\n",
    "    sdk_http_response=HttpResponse(\n",
    "    headers=<dict len=9>\n",
    "    ) tokens_info=[TokensInfo(\n",
    "    role='user',\n",
    "    token_ids=[\n",
    "        1841,\n",
    "        235303,\n",
    "        235256,\n",
    "        573,\n",
    "        32514,\n",
    "        <... 6 more items ...>,\n",
    "    ],\n",
    "    tokens=[\n",
    "        b'What',\n",
    "        b\"'\",\n",
    "        b's',\n",
    "        b' the',\n",
    "        b' longest',\n",
    "        <... 6 more items ...>,\n",
    "    ]\n",
    "    )]\n",
    "    ```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "* **참고: 이 메서드는 Vertex AI에서만 지원**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff592e4a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc39bc1",
   "metadata": {},
   "source": [
    "### **Grounding 검색**\n",
    "\n",
    "* [그라운딩(Grounding)](https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview) = 현실의 `데이터`를 `Gemini 모델`과 `연결`\n",
    "\n",
    "* 모델의 응답을 `Google 검색 결과에` `연결(grounding)` \n",
    "  * 모델은 훈련 데이터 범위를 넘어선 최신의 정확하고 관련성 있는 정보를 런타임에 접근하여 활용\n",
    "  * `Google 검색`을 활용한 그라운딩을 통해 모델 응답의 정확성과 최신성을 향상\n",
    "  * `Gemini 2.0부터`는 `Google 검색이 도구(tool)`로 `제공` = `모델`이 `Google 검색`을 언제 사용할지 `스스로 결정`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c05a8",
   "metadata": {},
   "source": [
    "#### **Google 검색**\n",
    "\n",
    "* `tools` \n",
    "  * 키워드 인수에 `GoogleSearch`를 포함한 `Tool`을 추가\n",
    "  * `Gemini`가 프롬프트에 대해 **먼저 `Google 검색`을 수행** → 웹 검색 결과를 기반으로 답변을 구성\n",
    "\n",
    "* [`동적 검색(Dynamic Retrieval)`](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-with-google-search#dynamic-retrieval)\n",
    "  * 모델 응답에 그라운딩을 사용하는 시점에 대한 임계값을 설정\n",
    "  * 프롬프트가 Google 검색 기반의 답변을 요구하지 않고, 지원되는 모델이 그라운딩 없이도 **자체 지식으로 답변을 제공할 수 있을 때 유용**\n",
    "  * 이를 통해 `지연 시간(latency)`, 품질 및 비용을 보다 효과적으로 관리 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=\"When is the next total solar eclipse in the United States?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[\n",
    "            types.Tool(\n",
    "                google_search=types.GoogleSearch()\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))\n",
    "\n",
    "print(response.candidates[0].grounding_metadata)\n",
    "\n",
    "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a0e9be",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (변수 조절 X)(2.3s)\n",
    "\n",
    "    ```markdown\n",
    "    The next total solar eclipse visible in the United States will occur on August 23, 2044. However, it will only be visible in Montana, North Dakota, and South Dakota.\n",
    "\n",
    "    Another total solar eclipse will occur on August 12, 2045, and it will be visible across the entire continental U.S., spanning from California to Florida.\n",
    "    ```\n",
    "\n",
    "    ```js\n",
    "    grounding_chunks=[GroundingChunk(\n",
    "    web=GroundingChunkWeb(\n",
    "        title='cbsnews.com',\n",
    "        uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFYEigGPHQTNcRbBnS0_8SsxRYabW-eaMWgzqieLf7yPhQmU0TdQd8L_jJY4OsB-OC3Vga1XYipCyQ7FNIzf48aSz1gXtG46YUmB96FuxIuQQH51Ho99wGgKStNJ-q6nu0Y63el81CCmsUYeFoH-bi6YwbBu9Jmj576mA=='\n",
    "    )\n",
    "    ), GroundingChunk(\n",
    "    web=GroundingChunkWeb(\n",
    "        title='space.com',\n",
    "        uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEltgDkd4qGBaYbZdaTlLdlHZJF-xmQydRD9rbAuMUD9yJqXrul76BrQBr8E6a7zdrWsTzDq0i7lDbrAPxAlwhSoYX69m1-CqtCS-Lu4ZMZ8cgK48_ugclPUYt5isFgWYj_pPINgOyeX6CZjEZwgtaP'\n",
    "    )\n",
    "    )] grounding_supports=[GroundingSupport(\n",
    "    confidence_scores=[\n",
    "        0.91026646,\n",
    "        0.019670822,\n",
    "    ],\n",
    "    grounding_chunk_indices=[\n",
    "        0,\n",
    "        1,\n",
    "    ],\n",
    "    segment=Segment(\n",
    "        end_index=87,\n",
    "        text='The next total solar eclipse visible in the United States will occur on August 23, 2044'\n",
    "    )\n",
    "    ), GroundingSupport(\n",
    "    confidence_scores=[\n",
    "        0.8805385,\n",
    "        0.032336947,\n",
    "    ],\n",
    "    grounding_chunk_indices=[\n",
    "        1,\n",
    "        0,\n",
    "    ],\n",
    "    segment=Segment(\n",
    "        end_index=164,\n",
    "        start_index=89,\n",
    "        text='However, it will only be visible in Montana, North Dakota, and South Dakota'\n",
    "    )\n",
    "    ), GroundingSupport(\n",
    "    confidence_scores=[\n",
    "        0.94996846,\n",
    "        0.0059847105,\n",
    "    ],\n",
    "    grounding_chunk_indices=[\n",
    "        1,\n",
    "        0,\n",
    "    ],\n",
    "    segment=Segment(\n",
    "        end_index=320,\n",
    "        start_index=167,\n",
    "        text='Another total solar eclipse will occur on August 12, 2045, and it will be visible across the entire continental U.S., spanning from California to Florida'\n",
    "    )\n",
    "    )] retrieval_metadata=RetrievalMetadata() retrieval_queries=None search_entry_point=SearchEntryPoint(\n",
    "    rendered_content=\"\"\"<style>\n",
    "    .container {\n",
    "    align-items: center;\n",
    "    border-radius: 8px;\n",
    "    display: flex;\n",
    "    font-family: Google Sans, Roboto, sans-serif;\n",
    "    font-size: 14px;\n",
    "    line-height: 20px;\n",
    "    padding: 8px 12px;\n",
    "    }\n",
    "    .chip {\n",
    "    display: inline-block;\n",
    "    border: solid 1px;\n",
    "    border-radius: 16px;\n",
    "    min-width: 14px;\n",
    "    padding: 5px 16px;\n",
    "    text-align: center;\n",
    "    user-select: none;\n",
    "    margin: 0 8px;\n",
    "    -webkit-tap-highlight-color: transparent;\n",
    "    }\n",
    "    .carousel {\n",
    "    overflow: auto;\n",
    "    scrollbar-width: none;\n",
    "    white-space: nowrap;\n",
    "    margin-right: -12px;\n",
    "    }\n",
    "    .headline {\n",
    "    display: flex;\n",
    "    margin-right: 4px;\n",
    "    }\n",
    "    .gradient-container {\n",
    "    position: relative;\n",
    "    }\n",
    "    .gradient {\n",
    "    position: absolute;\n",
    "    transform: translate(3px, -9px);\n",
    "    height: 36px;\n",
    "    width: 9px;\n",
    "    }\n",
    "    @media (prefers-color-scheme: light) {\n",
    "    .container {\n",
    "        background-color: #fafafa;\n",
    "        box-shadow: 0 0 0 1px #0000000f;\n",
    "    }\n",
    "    .headline-label {\n",
    "        color: #1f1f1f;\n",
    "    }\n",
    "    .chip {\n",
    "        background-color: #ffffff;\n",
    "        border-color: #d2d2d2;\n",
    "        color: #5e5e5e;\n",
    "        text-decoration: none;\n",
    "    }\n",
    "    .chip:hover {\n",
    "        background-color: #f2f2f2;\n",
    "    }\n",
    "    .chip:focus {\n",
    "        background-color: #f2f2f2;\n",
    "    }\n",
    "    .chip:active {\n",
    "        background-color: #d8d8d8;\n",
    "        border-color: #b6b6b6;\n",
    "    }\n",
    "    .logo-dark {\n",
    "        display: none;\n",
    "    }\n",
    "    .gradient {\n",
    "        background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
    "    }\n",
    "    }\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "    .container {\n",
    "        background-color: #1f1f1f;\n",
    "        box-shadow: 0 0 0 1px #ffffff26;\n",
    "    }\n",
    "    .headline-label {\n",
    "        color: #fff;\n",
    "    }\n",
    "    .chip {\n",
    "        background-color: #2c2c2c;\n",
    "        border-color: #3c4043;\n",
    "        color: #fff;\n",
    "        text-decoration: none;\n",
    "    }\n",
    "    .chip:hover {\n",
    "        background-color: #353536;\n",
    "    }\n",
    "    .chip:focus {\n",
    "        background-color: #353536;\n",
    "    }\n",
    "    .chip:active {\n",
    "        background-color: #464849;\n",
    "        border-color: #53575b;\n",
    "    }\n",
    "    .logo-light {\n",
    "        display: none;\n",
    "    }\n",
    "    .gradient {\n",
    "        background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
    "    }\n",
    "    }\n",
    "    </style>\n",
    "    <div class=\"container\">\n",
    "    <div class=\"headline\">\n",
    "        <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "        <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
    "        <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
    "        <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
    "        <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
    "        </svg>\n",
    "        <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "        <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
    "        <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
    "        <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
    "        <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
    "        <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
    "        </svg>\n",
    "        <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
    "    </div>\n",
    "    <div class=\"carousel\">\n",
    "        <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_UdYTzCMOInx6C-adQE3DSCrqpAMY9dVOZJjQKAXtLapXBrbe1oenOITWOecwdI3UfaFFjcfCQR4GSvrmylyXSQwH96zlPq2YLtTFbxFNSoTIw3rw3TGXhLmdVfzYleUSeIJARMviuvsGb8y2092k1PUcocY1wuf6PE-FYpb6QLJKKyRkzTiAtL2UmoTzrCQnVTgQNqpUu9mtUzfvNmQ6CHTR1ximZ5g2D-Wiow==\">next total solar eclipse United States date</a>\n",
    "    </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    ) web_search_queries=['next total solar eclipse United States date']\n",
    "    ```\n",
    "\n",
    "    ```css\n",
    "    <style>\n",
    "    .container {\n",
    "    align-items: center;\n",
    "    border-radius: 8px;\n",
    "    display: flex;\n",
    "    font-family: Google Sans, Roboto, sans-serif;\n",
    "    font-size: 14px;\n",
    "    line-height: 20px;\n",
    "    padding: 8px 12px;\n",
    "    }\n",
    "    .chip {\n",
    "    display: inline-block;\n",
    "    border: solid 1px;\n",
    "    border-radius: 16px;\n",
    "    min-width: 14px;\n",
    "    padding: 5px 16px;\n",
    "    text-align: center;\n",
    "    user-select: none;\n",
    "    margin: 0 8px;\n",
    "    -webkit-tap-highlight-color: transparent;\n",
    "    }\n",
    "    .carousel {\n",
    "    overflow: auto;\n",
    "    scrollbar-width: none;\n",
    "    white-space: nowrap;\n",
    "    margin-right: -12px;\n",
    "    }\n",
    "    .headline {\n",
    "    display: flex;\n",
    "    margin-right: 4px;\n",
    "    }\n",
    "    .gradient-container {\n",
    "    position: relative;\n",
    "    }\n",
    "    .gradient {\n",
    "    position: absolute;\n",
    "    transform: translate(3px, -9px);\n",
    "    height: 36px;\n",
    "    width: 9px;\n",
    "    }\n",
    "    @media (prefers-color-scheme: light) {\n",
    "    .container {\n",
    "        background-color: #fafafa;\n",
    "        box-shadow: 0 0 0 1px #0000000f;\n",
    "    }\n",
    "    .headline-label {\n",
    "        color: #1f1f1f;\n",
    "    }\n",
    "    .chip {\n",
    "        background-color: #ffffff;\n",
    "        border-color: #d2d2d2;\n",
    "        color: #5e5e5e;\n",
    "        text-decoration: none;\n",
    "    }\n",
    "    .chip:hover {\n",
    "        background-color: #f2f2f2;\n",
    "    }\n",
    "    .chip:focus {\n",
    "        background-color: #f2f2f2;\n",
    "    }\n",
    "    .chip:active {\n",
    "        background-color: #d8d8d8;\n",
    "        border-color: #b6b6b6;\n",
    "    }\n",
    "    .logo-dark {\n",
    "        display: none;\n",
    "    }\n",
    "    .gradient {\n",
    "        background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
    "    }\n",
    "    }\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "    .container {\n",
    "        background-color: #1f1f1f;\n",
    "        box-shadow: 0 0 0 1px #ffffff26;\n",
    "    }\n",
    "    .headline-label {\n",
    "        color: #fff;\n",
    "    }\n",
    "    .chip {\n",
    "        background-color: #2c2c2c;\n",
    "        border-color: #3c4043;\n",
    "        color: #fff;\n",
    "        text-decoration: none;\n",
    "    }\n",
    "    .chip:hover {\n",
    "        background-color: #353536;\n",
    "    }\n",
    "    .chip:focus {\n",
    "        background-color: #353536;\n",
    "    }\n",
    "    .chip:active {\n",
    "        background-color: #464849;\n",
    "        border-color: #53575b;\n",
    "    }\n",
    "    .logo-light {\n",
    "        display: none;\n",
    "    }\n",
    "    .gradient {\n",
    "        background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
    "    }\n",
    "    }\n",
    "    </style>\n",
    "    <div class=\"container\">\n",
    "    <div class=\"headline\">\n",
    "        <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "        <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
    "        <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
    "        <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
    "        <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
    "        </svg>\n",
    "        <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "        <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
    "        <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
    "        <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
    "        <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
    "        <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
    "        </svg>\n",
    "        <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
    "    </div>\n",
    "    <div class=\"carousel\">\n",
    "        <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF_UdYTzCMOInx6C-adQE3DSCrqpAMY9dVOZJjQKAXtLapXBrbe1oenOITWOecwdI3UfaFFjcfCQR4GSvrmylyXSQwH96zlPq2YLtTFbxFNSoTIw3rw3TGXhLmdVfzYleUSeIJARMviuvsGb8y2092k1PUcocY1wuf6PE-FYpb6QLJKKyRkzTiAtL2UmoTzrCQnVTgQNqpUu9mtUzfvNmQ6CHTR1ximZ5g2D-Wiow==\">next total solar eclipse United States date</a>\n",
    "    </div>\n",
    "    </div>\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8be20c",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 위 css로 생긴 버튼을 눌렀을 경우 구글 크롬 검색창으로 바로 이동 \n",
    "  * ![결과 이미지](../resources/result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515928c2",
   "metadata": {},
   "source": [
    "#### Vertex AI Search\n",
    "\n",
    "* [Vertex AI Search 데이터 스토어](https://cloud.google.com/generative-ai-app-builder/docs/create-data-store-es)를 사용하여 Gemini를 당신의 고유한 맞춤형 데이터와 연결 가능\n",
    "\n",
    "* 자세한 내용은 [Vertex AI Search 시작하기 가이드](https://cloud.google.com/generative-ai-app-builder/docs/try-enterprise-search)를 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e361b62",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6465ce",
   "metadata": {},
   "source": [
    "### 함수 호출(Function calling)\n",
    "\n",
    "* `Gemini`의 [함수 호출(Function Calling)](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) 기능\n",
    "  * 코드 내 함수의 설명을 작성한 다음, 해당 설명을 언어 모델에 요청으로 전달 가능\n",
    "  * `자동 함수 호출`을 위해 파이썬 함수를 제출 → `Gemini`가 함수를 `실행`하고 그 `결과`를 `자연어로 생성`하여 반환\n",
    "\n",
    "* [OpenAPI Specification](https://www.openapis.org/)을 제출 → 설명에 맞는 함수의 이름과 호출 시 사용할 인수를 응답으로 받을 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01452794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 호출\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"Get the current whether in a given location.\n",
    "\n",
    "    Args:\n",
    "        location: required, The city and state, e.g. San Franciso, CA\n",
    "        unit: celsius or fahrenheit\n",
    "    \"\"\"\n",
    "    print(f'Called with: {location=}')\n",
    "    return \"23C\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=\"What is the weather like in Boston?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[get_current_weather],\n",
    "        automatic_function_calling={'disable': True},\n",
    "    ),\n",
    ")\n",
    "\n",
    "function_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "print(function_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d6c780",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (변수 조절 X)(0.9s)\n",
    "\n",
    "    ```markdown\n",
    "    id=None args={'location': 'Boston, MA'} name='get_current_weather'\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15514054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=11>\n",
      ") candidates=[Candidate(\n",
      "  avg_logprobs=-0.023808644711971284,\n",
      "  content=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text=\"\"\"It is 23C in Boston.\n",
      "\"\"\"\n",
      "      ),\n",
      "    ],\n",
      "    role='model'\n",
      "  ),\n",
      "  finish_reason=<FinishReason.STOP: 'STOP'>\n",
      ")] create_time=None response_id=None model_version='gemini-2.0-flash' prompt_feedback=None usage_metadata=GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=10,\n",
      "  candidates_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=10\n",
      "    ),\n",
      "  ],\n",
      "  prompt_token_count=33,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=33\n",
      "    ),\n",
      "  ],\n",
      "  total_token_count=43\n",
      ") automatic_function_calling_history=[UserContent(\n",
      "  parts=[\n",
      "    Part(\n",
      "      text='What is the weather like in Boston?'\n",
      "    ),\n",
      "  ],\n",
      "  role='user'\n",
      "), Content(\n",
      "  parts=[\n",
      "    Part(\n",
      "      function_call=FunctionCall(\n",
      "        args={\n",
      "          'city': 'Boston'\n",
      "        },\n",
      "        name='get_current_weather'\n",
      "      )\n",
      "    ),\n",
      "  ],\n",
      "  role='model'\n",
      "), Content(\n",
      "  parts=[\n",
      "    Part(\n",
      "      function_response=FunctionResponse(\n",
      "        name='get_current_weather',\n",
      "        response={\n",
      "          'result': '23C'\n",
      "        }\n",
      "      )\n",
      "    ),\n",
      "  ],\n",
      "  role='user'\n",
      ")] parsed=None\n"
     ]
    }
   ],
   "source": [
    "# 자동 함수 호출\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "def get_current_weather(city: str) -> str:\n",
    "    return \"23C\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=\"What is the weather like in Boston?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[get_current_weather]\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "print(response.text)                        # It is 23C in Boston. (0.0s)\n",
    "\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2378693",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (변수 조절 X)(1.8s)\n",
    "\n",
    "    ```markdown\n",
    "    sdk_http_response=HttpResponse(\n",
    "    headers=<dict len=11>\n",
    "    ) candidates=[Candidate(\n",
    "    avg_logprobs=-0.023808644711971284,\n",
    "    content=Content(\n",
    "        parts=[\n",
    "        Part(\n",
    "            text=\"\"\"It is 23C in Boston.\n",
    "    \"\"\"\n",
    "        ),\n",
    "        ],\n",
    "        role='model'\n",
    "    ),\n",
    "    finish_reason=<FinishReason.STOP: 'STOP'>\n",
    "    )] create_time=None response_id=None model_version='gemini-2.0-flash' prompt_feedback=None usage_metadata=GenerateContentResponseUsageMetadata(\n",
    "    candidates_token_count=10,\n",
    "    candidates_tokens_details=[\n",
    "        ModalityTokenCount(\n",
    "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
    "        token_count=10\n",
    "        ),\n",
    "    ],\n",
    "    prompt_token_count=33,\n",
    "    prompt_tokens_details=[\n",
    "        ModalityTokenCount(\n",
    "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
    "        token_count=33\n",
    "        ),\n",
    "    ],\n",
    "    total_token_count=43\n",
    "    ) automatic_function_calling_history=[UserContent(\n",
    "    parts=[\n",
    "        Part(\n",
    "        text='What is the weather like in Boston?'\n",
    "        ),\n",
    "    ],\n",
    "    role='user'\n",
    "    ), Content(\n",
    "    parts=[\n",
    "        Part(\n",
    "        function_call=FunctionCall(\n",
    "            args={\n",
    "            'city': 'Boston'\n",
    "            },\n",
    "            name='get_current_weather'\n",
    "        )\n",
    "        ),\n",
    "    ],\n",
    "    role='model'\n",
    "    ), Content(\n",
    "    parts=[\n",
    "        Part(\n",
    "        function_response=FunctionResponse(\n",
    "            name='get_current_weather',\n",
    "            response={\n",
    "            'result': '23C'\n",
    "            }\n",
    "        )\n",
    "        ),\n",
    "    ],\n",
    "    role='user'\n",
    "    )] parsed=None\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "*  `generate_content()` 메서드가 반환하는 **응답 객체의 모든 값(속성)을 그대로 출력한 것**\n",
    "   * 파이썬 객체의 `문자열 표현(string representation)`\n",
    "   * `Gemini API`와 주고받은 통신에 대한 `모든 디테일`과 모델의 `최종 응답`\n",
    "\n",
    "---\n",
    "\n",
    "* 응답 객체 분석\n",
    "\n",
    "* 최종 응답 (`candidates` 섹션)\n",
    "\n",
    "  * **`candidates`**: 모델이 생성한 응답 후보들을 담고 있는 리스트\n",
    "  * **`content`**: 모델의 실제 응답 내용\n",
    "      * **`text=\"\"\"It is 23C in Boston.\"\"\"`**\n",
    "        * 모델이 최종적으로 생성하여 사용자에게 보여주는 자연어 텍스트\n",
    "        * 이 부분이 `response.text`를 통해 접근하는 값\n",
    "\n",
    "* 토큰 및 비용 정보 (`usage_metadata` 섹션)\n",
    "\n",
    "  * **`usage_metadata`**: API 사용량 및 비용 관련 정보를 제공\n",
    "      * **`prompt_token_count=33`**: 당신이 모델에 보낸 입력 프롬프트의 토큰 수\n",
    "      * **`candidates_token_count=10`**: 모델이 생성한 응답 텍스트의 토큰 수\n",
    "      * **`total_token_count=43`** = **총 사용된 토큰 수(입력 + 출력)** = 이 숫자를 기준으로 **비용 청구**\n",
    "\n",
    "* 함수 호출 기록 (`automatic_function_calling_history` 섹션)\n",
    "\n",
    "  * **`automatic_function_calling_history`**: 모델이 함수 호출을 수행한 과정에 대한 상세 기록\n",
    "      * **`role='user'`**: 사용자가 `\"What is the weather like in Boston?\"`이라고 질문\n",
    "      * **`role='model'`**: 모델이 내부적으로 `get_current_weather` 함수를 호출하기로 결정하고 `city` 인자로 `'Boston'`을 지정\n",
    "      * **`role='user'`**: (모델이 호출한 함수가 실행된 후) 그 결과값인 `{'result': '23C'}`가 다시 모델로 입력됨\n",
    "\n",
    "* **결론적으로, 이 출력은 Gemini가 당신의 질문을 이해하고, 날씨 정보를 가져오는 함수를 호출하여, 그 결과를 바탕으로 최종 답변 텍스트를 생성하는 모든 과정을 상세히 보여주는 '보고서'와 같음.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8715f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글 클라우드 실습 함수 예시로 시도\n",
    "\n",
    "def get_current_weather2(location: str) -> str:\n",
    "    \"\"\"Example method. Returns the current weather.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state, e.g. San Francisco, CA\n",
    "    \"\"\"\n",
    "    weather_map: dict[str, str] = {\n",
    "        \"Boston, MA\": \"snowing\",\n",
    "        \"San Francisco, CA\": \"foggy\",\n",
    "        \"Seattle, WA\": \"raining\",\n",
    "        \"Austin, TX\": \"hot\",\n",
    "        \"Chicago, IL\": \"windy\",\n",
    "    }\n",
    "    return weather_map.get(location, \"unknown\")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the weather like in Austin, TX?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[get_current_weather2],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26ad81",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력_1 (temperature=0)(2.3s)\n",
    "\n",
    "    ```markdown\n",
    "    Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
    "    Could you provide the state for Austin?\n",
    "    ```\n",
    "\n",
    "    ---\n",
    "\n",
    "    ```markdown\n",
    "    경고: 응답에 텍스트가 아닌 부분이 있습니다. ['thought_signature']에서 텍스트 부분만 연결하여 반환합니다. 전체 모델 응답을 보려면 `candidates.content.parts` 접근자를 확인하세요.\n",
    "\n",
    "    Austin의 주(state)를 알려주실 수 있나요?\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9a0ac",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력_2 (tempemrature=0)(2.2s)\n",
    "\n",
    "* 주 입력 요청에 따라 contents 수정 → contents=\"What is the weather like in Austin, TX?\",\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    The weather in Austin, TX is hot.\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e7dac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b022719",
   "metadata": {},
   "source": [
    "#### OpenAPI 상세화 (수동 함수 호출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2700c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "# --- client 객체를 사용하여 \"헬로우\" 호출해보기---\n",
    "# 1. 사용할 모델 지정 (gemini-2.5-flash-lite)\n",
    "\n",
    "# 2. '헬로우' 프롬프트로 콘텐츠 생성 요청\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.5-flash-lite',                     # 사용할 모델 지정 (gemini-2.5-flash-lite)\n",
    "        contents='hello'                                   # 요청하신 프롬프트 '헬로우'\n",
    "    )\n",
    "\n",
    "    # 응답 텍스트 출력\n",
    "    print(\"\\n--- 모델 응답 텍스트 ---\")\n",
    "    print(response.text)\n",
    "\n",
    "    # 응답 전체 내용(JSON 형식) 출력 (디버깅이나 상세 정보 확인용)\n",
    "    print(\"\\n--- 모델 응답 전체 JSON ---\")\n",
    "    print(response.model_dump_json(\n",
    "        exclude_none=True, indent=4))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n모델 호출 중 오류가 발생했습니다: {e}\")\n",
    "    print(\"다음 사항들을 확인해주세요:\")\n",
    "    print(\"1. 인터넷 연결 상태\")\n",
    "    print(\"2. GEMINI_API_KEY 환경 변수가 올바르고 유효한지\")\n",
    "    print(\"3. Google Cloud 프로젝트에서 Gemini API가 활성화되어 있는지\")\n",
    "    print(\"4. API 할당량이 초과되지 않았는지\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b53a8923",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - [{'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     10\u001b[39m tools = [\n\u001b[32m     11\u001b[39m     {\n\u001b[32m     12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     }\n\u001b[32m     47\u001b[39m ]\n\u001b[32m     49\u001b[39m MODEL_ID=\u001b[33m'\u001b[39m\u001b[33mgemini-2.5-flash\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m                              \u001b[49m\u001b[38;5;66;43;03m# chat.completions.create 사용\u001b[39;49;00m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mI\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43md like to travel to Paris.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 딕셔너리 리스트로 정의된 tools 사용\u001b[39;49;00m\n\u001b[32m     57\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.tool_calls[\u001b[32m0\u001b[39m].function)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/gcs_env/lib/python3.13/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/gcs_env/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1150\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1104\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1147\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1148\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1149\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/gcs_env/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/gcs_env/lib/python3.13/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - [{'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}]"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"GEMINI_API_KEY\",\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "\n",
    "# 모든 함수를 딕셔너리로 정의하기\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. Chicago, IL\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    # get_destination 함수를 딕셔너리 형태로 추가\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_destination\",\n",
    "            \"description\": \"Get the destination that the user wants to go to\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"destination\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Destination that the user wants to go to\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"destination\"],\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "MODEL_ID='gemini-2.5-flash'\n",
    "\n",
    "response = client.chat.completions.create(                              # chat.completions.create 사용\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"I'd like to travel to Paris.\"},\n",
    "    ],\n",
    "    tools=tools, # 딕셔너리 리스트로 정의된 tools 사용\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.tool_calls[0].function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77d1780b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tool' from 'google.genai' (/Users/jay/.pyenv/versions/gcs_env/lib/python3.13/site-packages/google/genai/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m genai\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tool, FunctionDeclaration\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. 새 SDK의 클라이언트 객체를 생성\u001b[39;00m\n\u001b[32m      5\u001b[39m client = genai.Client()\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'Tool' from 'google.genai' (/Users/jay/.pyenv/versions/gcs_env/lib/python3.13/site-packages/google/genai/__init__.py)"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import Tool, FunctionDeclaration\n",
    "\n",
    "# 1. 새 SDK의 클라이언트 객체를 생성\n",
    "client = genai.Client()\n",
    "\n",
    "# 2. 함수를 FunctionDeclaration 클래스로 정의\n",
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"destination\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# 3. Tool 객체에 함수 선언을 담기\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "# 4. client 객체를 통해 모델 호출\n",
    "# config에 tools를 전달하여 함수 호출을 활성화\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash', # 모델 ID\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=genai.types.GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 5. 함수 호출 결과 출력\n",
    "print(response.function_calls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "# 1. 새 SDK의 클라이언트 객체를 생성\n",
    "client = genai.Client()\n",
    "\n",
    "# 2. 함수 명세를 딕셔너리로 직접 정의\n",
    "# FunctionDeclaration과 Tool 클래스를 사용하지 않음\n",
    "destination_tool_spec = {\n",
    "    \"function_declarations\": [\n",
    "        {\n",
    "            \"name\": \"get_destination\",\n",
    "            \"description\": \"Get the destination that the user wants to go to\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": {\n",
    "                    \"destination\": {\n",
    "                        \"type\": \"STRING\",\n",
    "                        \"description\": \"Destination that the user wants to go to\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"destination\"],\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# 3. client 객체를 통해 모델 호출\n",
    "# tools 인수에 딕셔너리 명세를 직접 전달\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "        config=types.GenerateContentConfig(\n",
    "        tools=[destination_tool_spec],  \n",
    "    )\n",
    ")\n",
    "\n",
    "# 4. 함수 호출 결과 출력\n",
    "print(response.function_calls[0],'/n')\n",
    "print(response)\n",
    "print(destination_tool_spec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfed45e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "  ```markdown\n",
    "\n",
    "  id=None args={'destination': 'Paris'} name='get_destination'\n",
    "  \n",
    "  sdk_http_response=HttpResponse(\n",
    "    headers=<dict len=11>\n",
    "  ) candidates=[Candidate(\n",
    "    avg_logprobs=3.701401874423027e-06,\n",
    "    content=Content(\n",
    "      parts=[\n",
    "        Part(\n",
    "          function_call=FunctionCall(\n",
    "            args={\n",
    "              'destination': 'Paris'\n",
    "            },\n",
    "            name='get_destination'\n",
    "          )\n",
    "        ),\n",
    "      ],\n",
    "      role='model'\n",
    "    ),\n",
    "    finish_reason=<FinishReason.STOP: 'STOP'>\n",
    "  )] create_time=None response_id=None model_version='gemini-2.0-flash' prompt_feedback=None usage_metadata=GenerateContentResponseUsageMetadata(\n",
    "    candidates_token_count=5,\n",
    "    candidates_tokens_details=[\n",
    "      ModalityTokenCount(\n",
    "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
    "        token_count=5\n",
    "      ),\n",
    "    ],\n",
    "    prompt_token_count=34,\n",
    "    prompt_tokens_details=[\n",
    "      ModalityTokenCount(\n",
    "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
    "        token_count=34\n",
    "      ),\n",
    "    ],\n",
    "    total_token_count=39\n",
    "  ) automatic_function_calling_history=[] parsed=None\n",
    "  {'function_declarations': [{'name': 'get_destination', 'description': 'Get the destination that the user wants to go to', 'parameters': {'type': 'OBJECT', 'properties': {'destination': {'type': 'STRING', 'description': 'Destination that the user wants to go to'}}, 'required': ['destination']}}]}\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda78aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28f61ab7",
   "metadata": {},
   "source": [
    "#### next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c237f449",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Function' from 'google.genai.types' (/Users/jay/.pyenv/versions/gcs_env/lib/python3.13/site-packages/google/genai/types.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m genai\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Function, UserMessage\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      5\u001b[39m client = genai.Client()                 \n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'Function' from 'google.genai.types' (/Users/jay/.pyenv/versions/gcs_env/lib/python3.13/site-packages/google/genai/types.py)"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai.types import Function, UserMessage\n",
    "import json\n",
    "\n",
    "client = genai.Client()                 \n",
    "\n",
    "# 함수 선언\n",
    "get_weather = Function(\n",
    "    name=\"get_weather\",\n",
    "    description=\"전달된 위치의 날씨를 가져옵니다\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\"type\": \"string\", \"description\": \"도시 이름, 예: Seoul\"},\n",
    "            \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# 함수 호출 요청\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",  \n",
    "    messages=[UserMessage(content=\"서울의 날씨 알려줘\")],\n",
    "    functions=[get_weather],\n",
    "    function_call=\"auto\"\n",
    ")\n",
    "\n",
    "# 함수 호출 응답 확인\n",
    "if response.choices[0].message.function_call:\n",
    "    name = response.choices[0].message.function_call.name\n",
    "    arguments = response.choices[0].message.function_call.arguments\n",
    "    args_dict = json.loads(arguments)\n",
    "    print(\"함수 호출 이름:\", name)\n",
    "    print(\"인자:\", args_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98e4fa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called with: location=Seoul, unit=celsius\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m part = response.candidates[\u001b[32m0\u001b[39m].content.parts[\u001b[32m0\u001b[39m]\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(part, \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m함수 호출:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mpart\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m)\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m인자:\u001b[39m\u001b[33m\"\u001b[39m, part.function_call.args)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "# 함수 정의\n",
    "def get_current_weather(location: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"현재 날씨를 가져오는 함수\"\"\"\n",
    "    print(f\"Called with: location={location}, unit={unit}\")\n",
    "    return \"23°C and sunny\"\n",
    "\n",
    "# Gemini 모델에 함수 전달\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"What's the weather in Seoul?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[get_current_weather],  # Python 함수 직접 등록\n",
    "        automatic_function_calling={\"disable\": False},  # 자동 호출 허용\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 모델이 함수 호출을 했는지 확인\n",
    "part = response.candidates[0].content.parts[0]\n",
    "if hasattr(part, \"function_call\"):\n",
    "    print(\"함수 호출:\", part.function_call.name)\n",
    "    print(\"인자:\", part.function_call.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54caa516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called with: location=Seoul, unit=celsius\n",
      "모델 텍스트 응답: It's 23°C and sunny in Seoul.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "def get_current_weather(location: str, unit: str = \"celsius\") -> str:\n",
    "    print(f\"Called with: location={location}, unit={unit}\")\n",
    "    return \"23°C and sunny\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"What's the weather in Seoul?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[get_current_weather],\n",
    "        automatic_function_calling={\"disable\": False},  # 자동 함수 호출 허용\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 응답 확인\n",
    "for candidate in response.candidates:\n",
    "    for part in candidate.content.parts:\n",
    "        if part.function_call:\n",
    "            print(\"함수 호출됨 ✅\")\n",
    "            print(\"함수 이름:\", part.function_call.name)\n",
    "            print(\"인자:\", part.function_call.args)\n",
    "        elif part.text:\n",
    "            print(\"모델 텍스트 응답:\", part.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9aa3b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (1.4s)\n",
    "\n",
    "    ```markdown\n",
    "    Called with: location=Seoul, unit=celsius\n",
    "    모델 텍스트 응답: It's 23°C and sunny in Seoul.\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74339e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc943f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e0046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f6f083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
